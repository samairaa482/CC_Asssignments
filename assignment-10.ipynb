{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71faf4f3",
   "metadata": {},
   "source": [
    "##  Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04619cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499636d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading puntk: Package 'puntk' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('puntk')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dcb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8abd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"This is a test sentence! It's designed to show how Python's split() compares with NLTK's word_tokenize(). \n",
    "Also, note that some stopwords like 'the', 'is', and 'to' will be filtered out.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dee8fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is a test sentence it's designed to show how python's split compares with nltk's word_tokenize \\nalso note that some stopwords like 'the' 'is' and 'to' will be filtered out\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert to lowercase and remove punctuation using re\n",
    "cleaned_text = re.sub(r'[^\\w\\s\\']','',text1.lower())\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc01af9",
   "metadata": {},
   "source": [
    "Exp :\n",
    "text.lower()-> convert the entire text into lower case\n",
    "re.sub()-> reg expression substitute function...replaces all the matches of the pattern in the string with the replacement\n",
    "pattern : r'[^\\w\\s\\']' -> char class in []\n",
    "^->negates the class...char not in class\n",
    "\\w-> word char : letters,alphas and _\n",
    "\\s-> whitespaces\n",
    "\\' -> apostrophe matches\n",
    "Thus, remove anything that is not a letter, digit,_,space or '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8039d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', '!', 'It', \"'s\", 'designed', 'to', 'show', 'how', 'Python', \"'s\", 'split', '(', ')', 'compares', 'with', 'NLTK', \"'s\", 'word_tokenize', '(', ')', '.', 'Also', ',', 'note', 'that', 'some', 'stopwords', 'like', \"'the\", \"'\", ',', \"'is\", \"'\", ',', 'and', \"'to\", \"'\", 'will', 'be', 'filtered', 'out', '.']\n",
      "\n",
      "\n",
      "['This is a test sentence!', \"It's designed to show how Python's split() compares with NLTK's word_tokenize().\", \"Also, note that some stopwords like 'the', 'is', and 'to' will be filtered out.\"]\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenize into words and sentences\n",
    "words=word_tokenize(text1)\n",
    "sents=sent_tokenize(text1)\n",
    "print(words)\n",
    "print(\"\\n\")\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66bd031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test', 'sentence', \"it's\", 'designed', 'to', 'show', 'how', \"python's\", 'split', 'compares', 'with', \"nltk's\", 'word_tokenize', 'also', 'note', 'that', 'some', 'stopwords', 'like', \"'the'\", \"'is'\", 'and', \"'to'\", 'will', 'be', 'filtered', 'out']\n",
      "['This', 'is', 'a', 'test', 'sentence', '!', 'It', \"'s\", 'designed', 'to', 'show', 'how', 'Python', \"'s\", 'split', '(', ')', 'compares', 'with', 'NLTK', \"'s\", 'word_tokenize', '(', ')', '.', 'Also', ',', 'note', 'that', 'some', 'stopwords', 'like', \"'the\", \"'\", ',', \"'is\", \"'\", ',', 'and', \"'to\", \"'\", 'will', 'be', 'filtered', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# 3. Compare split() vs word_tokenize()\n",
    "split_words=cleaned_text.split() #ise punctuation, contraction wagera nahi samjh aati\n",
    "tokenized_words=word_tokenize(text1)\n",
    "print(split_words)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9faf6002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'sentence', '!', \"'s\", 'designed', 'show', 'Python', \"'s\", 'split', '(', ')', 'compares', 'NLTK', \"'s\", 'word_tokenize', '(', ')', '.', 'Also', ',', 'note', 'stopwords', 'like', \"'the\", \"'\", ',', \"'is\", \"'\", ',', \"'to\", \"'\", 'filtered', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Remove stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words=[word for word in tokenized_words if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6792c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s:3\n",
      ",:3\n",
      "':3\n",
      "(:2\n",
      "):2\n",
      ".:2\n",
      "test:1\n",
      "sentence:1\n",
      "!:1\n",
      "designed:1\n",
      "show:1\n",
      "Python:1\n",
      "split:1\n",
      "compares:1\n",
      "NLTK:1\n",
      "word_tokenize:1\n",
      "Also:1\n",
      "note:1\n",
      "stopwords:1\n",
      "like:1\n",
      "'the:1\n",
      "'is:1\n",
      "'to:1\n",
      "filtered:1\n"
     ]
    }
   ],
   "source": [
    "# 5. Word frequency distribution (excluding stopwords)\n",
    "from nltk.probability import FreqDist\n",
    "word_freq=FreqDist(filtered_words)\n",
    "for word,freq in word_freq.most_common():\n",
    "    print(f\"{word}:{freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822637dc",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc16350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9ee5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence',\n",
       " 'It',\n",
       " 's',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'show',\n",
       " 'how',\n",
       " 'Python',\n",
       " 's',\n",
       " 'split',\n",
       " 'compares',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " 's',\n",
       " 'word_tokenize',\n",
       " 'Also',\n",
       " 'note',\n",
       " 'that',\n",
       " 'some',\n",
       " 'stopwords',\n",
       " 'like',\n",
       " 'the',\n",
       " 'is',\n",
       " 'and',\n",
       " 'to',\n",
       " 'will',\n",
       " 'be',\n",
       " 'filtered',\n",
       " 'out']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Extract words with only alphabets using re.findall():\n",
    "alpha_words= re.findall(r'\\b[a-zA-z]+\\b',text1)\n",
    "alpha_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be05b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a622a02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'sentence',\n",
       " 'designed',\n",
       " 'show',\n",
       " 'Python',\n",
       " 'split',\n",
       " 'compares',\n",
       " 'NLTK',\n",
       " 'word_tokenize',\n",
       " 'Also',\n",
       " 'note',\n",
       " 'stopwords',\n",
       " 'like',\n",
       " 'filtered']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Remove stopwords using NLTK:\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filt=[word for word in alpha_words if word.lower() not in stop_words]\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4024a9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'sentenc',\n",
       " 'design',\n",
       " 'show',\n",
       " 'python',\n",
       " 'split',\n",
       " 'compar',\n",
       " 'nltk',\n",
       " 'word_token',\n",
       " 'also',\n",
       " 'note',\n",
       " 'stopword',\n",
       " 'like',\n",
       " 'filter']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Perform stemming with PorterStemmer:\n",
    "porter=PorterStemmer()\n",
    "stemmed_words=[porter.stem(word) for word in filt]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e422f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf39015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'sentence',\n",
       " 'designed',\n",
       " 'show',\n",
       " 'python',\n",
       " 'split',\n",
       " 'compare',\n",
       " 'nltk',\n",
       " 'word_tokenize',\n",
       " 'also',\n",
       " 'note',\n",
       " 'stopwords',\n",
       " 'like',\n",
       " 'filtered']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm=WordNetLemmatizer()\n",
    "words_lemmatized=[lemm.lemmatize(word.lower()) for word in filt]\n",
    "words_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3599e35",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c6e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Apple launches new iphone with amazing camera features\",\n",
    "         \"Samsung's foldable phone receives mixed reviews from users\",\n",
    "         \"Google unveils AI tools to boost productivity in workspace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ece8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'amazing' 'apple' 'boost' 'camera' 'features' 'foldable' 'from'\n",
      " 'google' 'in' 'iphone' 'launches' 'mixed' 'new' 'phone' 'productivity'\n",
      " 'receives' 'reviews' 'samsung' 'to' 'tools' 'unveils' 'users' 'with'\n",
      " 'workspace']\n",
      "[[0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Bag of Words with CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix =count_vectorizer.fit_transform(texts)\n",
    "\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_matrix.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ecf287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.35355339 0.35355339 0.         0.35355339 0.35355339\n",
      "  0.         0.         0.         0.         0.35355339 0.35355339\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339 0.35355339 0.         0.         0.         0.\n",
      "  0.35355339 0.         0.35355339 0.         0.35355339 0.35355339\n",
      "  0.35355339 0.         0.         0.         0.35355339 0.\n",
      "  0.        ]\n",
      " [0.33333333 0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.33333333 0.33333333 0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.33333333 0.33333333 0.33333333 0.         0.\n",
      "  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "#2. TF-IDF with TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "tfdif_vectorizer=TfidfVectorizer()\n",
    "tfdif_matrix=tfdif_vectorizer.fit_transform(texts)\n",
    "feature_names=tfdif_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(tfdif_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef784e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 1: Apple launches new iphone with amazing camera features\n",
      " - iphone: 0.3536\n",
      " - amazing: 0.3536\n",
      " - apple: 0.3536\n",
      "\n",
      "Text 2: Samsung's foldable phone receives mixed reviews from users\n",
      " - mixed: 0.3536\n",
      " - users: 0.3536\n",
      " - samsung: 0.3536\n",
      "\n",
      "Text 3: Google unveils AI tools to boost productivity in workspace\n",
      " - workspace: 0.3333\n",
      " - productivity: 0.3333\n",
      " - boost: 0.3333\n"
     ]
    }
   ],
   "source": [
    "#3. Top 3 Keywords per Text using TF-IDF\n",
    "for i, row in enumerate(tfdif_matrix):\n",
    "    print(f\"\\nText {i+1}: {texts[i]}\")\n",
    "    row_array = row.toarray().flatten()\n",
    "    top_indices = row_array.argsort()[-3:][::-1]  # top 3 indices by score\n",
    "    for idx in top_indices:\n",
    "        print(f\" - {feature_names[idx]}: {row_array[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a92082",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83d28c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ai = \"\"\"\n",
    "Artificial Intelligence (AI) enables machines to learn from data and perform tasks intelligently.\n",
    "It powers voice assistants, recommendation systems, and self-driving cars.\n",
    "AI is transforming industries by automating repetitive tasks and enhancing decision-making.\n",
    "\"\"\"\n",
    "\n",
    "text_blockchain = \"\"\"\n",
    "Blockchain is a decentralized ledger technology that ensures transparency and security.\n",
    "It is used in cryptocurrency systems like Bitcoin and Ethereum.\n",
    "Blockchain helps prevent fraud and build trust in digital transactions.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (AI): ['artificial', 'intelligence', 'ai', 'enables', 'machines', 'learn', 'data', 'perform', 'tasks', 'intelligently', 'powers', 'voice', 'assistants', 'recommendation', 'systems', 'selfdriving', 'cars', 'ai', 'transforming', 'industries', 'automating', 'repetitive', 'tasks', 'enhancing', 'decisionmaking']\n",
      "Tokens (Blockchain): ['blockchain', 'decentralized', 'ledger', 'technology', 'ensures', 'transparency', 'security', 'used', 'cryptocurrency', 'systems', 'like', 'bitcoin', 'ethereum', 'blockchain', 'helps', 'prevent', 'fraud', 'build', 'trust', 'digital', 'transactions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4118708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (AI): ['artificial', 'intelligence', 'ai', 'enables', 'machines', 'learn', 'data', 'perform', 'tasks', 'intelligently', 'powers', 'voice', 'assistants', 'recommendation', 'systems', 'selfdriving', 'cars', 'ai', 'transforming', 'industries', 'automating', 'repetitive', 'tasks', 'enhancing', 'decisionmaking']\n",
      "Tokens (Blockchain): ['blockchain', 'decentralized', 'ledger', 'technology', 'ensures', 'transparency', 'security', 'used', 'cryptocurrency', 'systems', 'like', 'bitcoin', 'ethereum', 'blockchain', 'helps', 'prevent', 'fraud', 'build', 'trust', 'digital', 'transactions']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return filtered\n",
    "\n",
    "tokens_ai = preprocess(text_ai)\n",
    "tokens_blockchain = preprocess(text_blockchain)\n",
    "\n",
    "print(\"Tokens (AI):\", tokens_ai)\n",
    "print(\"Tokens (Blockchain):\", tokens_blockchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9e16414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard Similarity: 0.0238\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "set_ai = set(tokens_ai)\n",
    "set_blockchain = set(tokens_blockchain)\n",
    "jaccard_sim = jaccard_similarity(set_ai, set_blockchain)\n",
    "print(f\"\\nJaccard Similarity: {jaccard_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4663fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.0193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform([text_ai, text_blockchain])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e3d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Analysis:\n",
      "Jaccard similarity is more helpful if you're interested in pure word overlap.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“Š Analysis:\")\n",
    "if cosine_sim > jaccard_sim:\n",
    "    print(\"Cosine similarity gives better insight here because it captures word frequency and importance.\")\n",
    "    print(\"Jaccard only considers shared vocabulary and ignores context or weighting.\")\n",
    "else:\n",
    "    print(\"Jaccard similarity is more helpful if you're interested in pure word overlap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19008b",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53db9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"I absolutely love this phone! The camera quality is amazing and the battery lasts all day.\",\n",
    "    \"The product is okay, but the delivery was late and packaging was poor.\",\n",
    "    \"Terrible experience. The app keeps crashing and customer support was not helpful.\",\n",
    "    \"Decent service, nothing special but it works fine for me.\",\n",
    "    \"Excellent quality and very fast shipping. Highly recommend!\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79dafe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely love this phone! The camera quality is amazing and the battery lasts all day.\n",
      "Polarity: 0.862, Sentiment: Positive\n",
      "\n",
      "Review: The product is okay, but the delivery was late and packaging was poor.\n",
      "Polarity: -0.5719, Sentiment: Negative\n",
      "\n",
      "Review: Terrible experience. The app keeps crashing and customer support was not helpful.\n",
      "Polarity: -0.4082, Sentiment: Negative\n",
      "\n",
      "Review: Decent service, nothing special but it works fine for me.\n",
      "Polarity: 0.1459, Sentiment: Positive\n",
      "\n",
      "Review: Excellent quality and very fast shipping. Highly recommend!\n",
      "Polarity: 0.7773, Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "results = []\n",
    "for review in reviews:\n",
    "    score = sia.polarity_scores(review)\n",
    "    polarity = score['compound']  # Compound score between -1 (neg) and +1 (pos)\n",
    "    # Classify based on polarity thresholds\n",
    "    if polarity >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif polarity <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    results.append({'review': review, 'polarity': polarity, 'sentiment': sentiment})\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Review: {res['review']}\\nPolarity: {res['polarity']}, Sentiment: {res['sentiment']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5705d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine all positive reviews into one text\n",
    "positive_reviews = \" \".join([res['review'] for res in results if res['sentiment'] == 'Positive'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Positive Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890f1f3",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afa02449",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Artificial Intelligence is transforming the world rapidly. From healthcare to finance, it enables smarter decisions. Machine learning algorithms learn from data patterns to improve accuracy. Natural language processing helps machines understand human languages. Deep learning models power advancements in computer vision and speech recognition. As technology evolves, AI continues to create new opportunities and challenges. Ethical considerations are vital to ensure AI benefits everyone. Collaborative efforts among researchers and developers accelerate progress. The future promises more intelligent systems integrated into daily life.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c36ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer and fit on text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Total unique words\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(f\"Total words: {total_words}\")\n",
    "\n",
    "# Convert text to sequence of tokens\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Create input sequences using the tokens, e.g.:\n",
    "# For tokens [1,2,3,4], create sequences: [1,2], [1,2,3], [1,2,3,4]\n",
    "input_sequences = []\n",
    "for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences so all are the same length\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "print(f\"Example input sequence (padded): {input_sequences[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequences into input (all words except last) and label (last word)\n",
    "input_sequences = np.array(input_sequences)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# One-hot encode labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a03363",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words=10):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs, axis=1)[0]\n",
    "        \n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        seed_text += ' ' + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Example usage:\n",
    "print(generate_text(\"artificial intelligence\", next_words=15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
