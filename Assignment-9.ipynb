{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e446ba0e",
   "metadata": {},
   "source": [
    "## Assignment-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef566c",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36669edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae300960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65abe6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a33e05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"\"\"Hi there! Today we learnt about NLP. It was really fun learning NLP. We will be continuing the lesson shortly. Meanwhile, we are thinking to practice writing the codes for the same .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af129956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(text):\n",
    "    text=text.lower() #converts to lowercase\n",
    "    tokens=word_tokenize(text) # Tokenize\n",
    "    tokens = [word for word in tokens if word not in string.punctuation] #Removes the punctuation\n",
    "    return ''.join(tokens) #parse->split->remove punc->recombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c74e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(preprocess_test(text1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f2fd95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hitheretodaywelearntaboutnlpitwasreallyfunlearningnlpwewillbecontinuingthelessonshortlymeanwhilewearethinkingtopracticewritingthecodesforthesame\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "807daecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there', '!', 'Today', 'we', 'learnt', 'about', 'NLP', '.', 'It', 'was', 'really', 'fun', 'learning', 'NLP', '.', 'We', 'will', 'be', 'continuing', 'the', 'lesson', 'shortly', '.', 'Meanwhile', ',', 'we', 'are', 'thinking', 'to', 'practice', 'writing', 'the', 'codes', 'for', 'the', 'same', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(text1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f675b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd70d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi there!', 'Today we learnt about NLP.', 'It was really fun learning NLP.', 'We will be continuing the lesson shortly.', 'Meanwhile, we are thinking to practice writing the codes for the same .']\n"
     ]
    }
   ],
   "source": [
    "sent=sent_tokenize(text1)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aa94b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ef07ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc64f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'Today', 'learnt', 'NLP', '.', 'really', 'fun', 'learning', 'NLP', '.', 'continuing', 'lesson', 'shortly', '.', 'Meanwhile', ',', 'thinking', 'practice', 'writing', 'codes', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(text1)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7be4177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4860b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8338cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:4\n",
      "NLP:2\n",
      "Hi:1\n",
      "!:1\n",
      "Today:1\n",
      "learnt:1\n",
      "really:1\n",
      "fun:1\n",
      "learning:1\n",
      "continuing:1\n",
      "lesson:1\n",
      "shortly:1\n",
      "Meanwhile:1\n",
      ",:1\n",
      "thinking:1\n",
      "practice:1\n",
      "writing:1\n",
      "codes:1\n"
     ]
    }
   ],
   "source": [
    "for word,freq in freq_dist.most_common():\n",
    "    print(f\"{word}:{freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a9d80",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b95708b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer,RegexpStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8c894011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the stemmers\n",
    "porter= PorterStemmer() #less aggressive\n",
    "lancaster=LancasterStemmer() #more aggressive and may shorten words more drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ac81441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the stemmer\n",
    "porter_stemmed = [porter.stem(word) for word in filtered_tokens]\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in filtered_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7bfb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freq Dist\n",
    "porter_freq=FreqDist(porter_stemmed)\n",
    "lancaster_freq=FreqDist(lancaster_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58a4f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:4\n",
      "nlp:2\n",
      "hi:1\n",
      "!:1\n",
      "today:1\n",
      "learnt:1\n",
      "realli:1\n",
      "fun:1\n",
      "learn:1\n",
      "continu:1\n",
      "lesson:1\n",
      "shortli:1\n",
      "meanwhil:1\n",
      ",:1\n",
      "think:1\n",
      "practic:1\n",
      "write:1\n",
      "code:1\n",
      "\n",
      "\n",
      ".:4\n",
      "nlp:2\n",
      "hi:1\n",
      "!:1\n",
      "today:1\n",
      "learnt:1\n",
      "real:1\n",
      "fun:1\n",
      "learn:1\n",
      "continu:1\n",
      "lesson:1\n",
      "short:1\n",
      "meanwhil:1\n",
      ",:1\n",
      "think:1\n",
      "pract:1\n",
      "writ:1\n",
      "cod:1\n"
     ]
    }
   ],
   "source": [
    "# Printing\n",
    "for word,freq in porter_freq.most_common():\n",
    "    print(f\"{word}:{freq}\")\n",
    "print(\"\\n\")\n",
    "for word,freq in lancaster_freq.most_common():\n",
    "    print(f\"{word}:{freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c39cef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ad19995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a8a90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:4\n",
      "NLP:2\n",
      "Hi:1\n",
      "!:1\n",
      "Today:1\n",
      "learnt:1\n",
      "really:1\n",
      "fun:1\n",
      "learning:1\n",
      "continuing:1\n",
      "lesson:1\n",
      "shortly:1\n",
      "Meanwhile:1\n",
      ",:1\n",
      "thinking:1\n",
      "practice:1\n",
      "writing:1\n",
      "code:1\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens=[lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "lemma_freq = FreqDist(lemmatized_tokens)\n",
    "\n",
    "for word,freq in lemma_freq.most_common():\n",
    "    print(f\"{word}:{freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10b0cd",
   "metadata": {},
   "source": [
    "## By default,lemmatizer.lemmatize(word) assumes noun\n",
    "**Extra : POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b0dc584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a615770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40576c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Map POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01ebceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhool mat jaana ise karna\n"
     ]
    }
   ],
   "source": [
    "print(\"Bhool mat jaana ise karna\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeaeb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abhi ke liye comparing wala samjh nahi aa raha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce09bd",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97673247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7709c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words =[word for word in tokens if word.isalpha() and len(word)>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7474d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learnt', 'really', 'learning', 'continuing', 'lesson', 'shortly', 'Meanwhile', 'thinking', 'practice', 'writing']\n"
     ]
    }
   ],
   "source": [
    "print(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "36d5e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers=[word for word in tokens if re.fullmatch(r'\\d+(\\.\\d+)?',word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e839391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_words=[word for word in tokens if word.isupper() and word.isalpha()] #all letters capital..if only first letter capital required then use word[0].isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e3baf7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "print(cap_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec2280b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_only=[word.lower() for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2d845f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'there',\n",
       " 'today',\n",
       " 'we',\n",
       " 'learnt',\n",
       " 'about',\n",
       " 'nlp',\n",
       " 'it',\n",
       " 'was',\n",
       " 'really',\n",
       " 'fun',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'we',\n",
       " 'will',\n",
       " 'be',\n",
       " 'continuing',\n",
       " 'the',\n",
       " 'lesson',\n",
       " 'shortly',\n",
       " 'meanwhile',\n",
       " 'we',\n",
       " 'are',\n",
       " 'thinking',\n",
       " 'to',\n",
       " 'practice',\n",
       " 'writing',\n",
       " 'the',\n",
       " 'codes',\n",
       " 'for',\n",
       " 'the',\n",
       " 'same']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "54b67f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words starting with vowels\n",
    "vowel_words=[word for word in tokens if word.isalpha() and word[0].lower() in 'aeiou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "840d6187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about', 'It', 'are']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowel_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a9007",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625075cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"\"\"This is a state-of-the-art product! It isn't cheap: costs about 3.14 dollars.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8912a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_contractions(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text while preserving contractions (e.g., \"isn't\").\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "\n",
    "def tokenize_hyphenated_words(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text while preserving hyphenated words (e.g., \"state-of-the-art\").\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b[a-zA-Z]+(?:-[a-zA-Z]+)+\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "\n",
    "def tokenize_decimal_numbers(text):\n",
    "    \"\"\"\n",
    "    Tokenizes numbers, preserving decimal numbers (e.g., \"3.14\").\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b\\d+\\.\\d+|\\b\\d+\\b\"\n",
    "    return re.findall(pattern, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ec1c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractions: ['This', 'is', 'a', 'state', 'of', 'the', 'art', 'product', 'It', \"isn't\", 'cheap', 'costs', 'about', 'dollars']\n",
      "Hyphenated Words: ['state-of-the-art']\n",
      "Numbers: ['3.14']\n"
     ]
    }
   ],
   "source": [
    "#a.\n",
    "print(\"Contractions:\", tokenize_contractions(text2))\n",
    "\n",
    "# b. Hyphenated words\n",
    "print(\"Hyphenated Words:\", tokenize_hyphenated_words(text2))\n",
    "\n",
    "# c. Numbers\n",
    "print(\"Numbers:\", tokenize_decimal_numbers(text2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
